---
knit: "bookdown::render_book"
---

# 05: Probabilidade condicional (continuação) {-}

## Vídeo {-}

```{r echo=FALSE, results='asis', out.extra=center()}
embed_yt('JzDvVgNDxo8')
```


## Exercícios do livro (cap. 2) {-}

### 30. Filho mais velho {-}

::: {.rmdbox latex=1}

Uma família tem $3$ filhos: $A$, $B$ e $C$.

a. O evento "$A$ é mais velho que $B$" é independente de "$A$ é mais velho que $C$"?

:::

* Intuitivamente: 

  Não, pois $A$ ser mais velho que $B$ torna mais provável que $A$ seja mais velho que $C$.

::: {.rmdbox latex=1}

b. Qual a probabilidade de que $A$ é mais velho que $B$, dado que $A$ é mais velho que $C$?

:::

* Queremos achar $P(A > B \mid A > C)$.

* Esta probabilidade é igual a 

  $$
  \frac{P(A > B,  A > C)}{P(A > C)}
  $$

* O numerador é a probabilidade de $A$ ser o mais velho.

* Se todas as $6$ ordens de nascimento tiverem a mesma probabilidade,

  $$
  P(A > B,  A > C) = 2/6 = 1/3
  $$

* O denominador é

  $$
  P(A > C) = 3/6 = 1/2
  $$

* Daí, $P(A > B \mid A > C) = \frac{1/3}{1/2} = \frac23$.

* De fato, a probabilidade [condicional]{.hl} $P(A > B \mid A > C) = 2/3$ é [maior]{.hl} do que a probabilidade [não-condicional]{.hl} $P(A > B) = 1/2$.


### 31. Auto-independência? {-}

::: {.rmdbox latex=1}

Um evento pode ser independente de si mesmo?

:::

* Chamando este evento de $A$, é preciso que

  $$
  P(A \cap A) = P(A) = P(A) \cdot P(A)
  $$

* Isto só é possível se $P(A) = 0$ ou se $P(A) = 1$.


### 32. Dados de Efron {-}

::: {.rmdbox latex=1}

Considere $4$ dados não-padrão (*dados de Efron*), cujos lados são rotulados da seguinte forma (cada lado tem a mesma probabilidade):

$$
\begin{aligned}
  A &: 4, 4, 4, 4, 0, 0 \\
  B &: 3, 3, 3, 3, 3, 3 \\
  C &: 6, 6, 2, 2, 2, 2 \\
  D &: 5, 5, 5, 1, 1, 1
\end{aligned}
$$

Cada dado é lançado uma vez. Cada letra representa o resultado do dado correspondente.

a. Ache $P(A > B)$, $P(B > C)$, $P(C > D)$, e $P(D > A)$.

:::

* Eventos equivalentes:

  $$
  \begin{aligned}
    A > B &\iff A = 4 \\
    B > C &\iff C = 2 \\
    C > D &\iff C = 6 \cup (C = 2 \cap D = 1) \\
    D > A & \iff D = 5 \cup (D = 1\cap A = 0)
  \end{aligned}
  $$

* $P(A > B) = 2/3$.

* $P(B > C) = 2/3$.

* $P(C > D) = 1/3 + 2/3 \cdot 1/2 = 2/3$.

* $P(D > A) = 1/2 + 1/2 \cdot 2/3 = 2/3$.

::: {.rmdbox latex=1}

b. O evento $A > B$ é independente de $B > C$?

   O evento $B > C$ é independente de $C > D$?

:::

* Sim: 

  $$
  \begin{aligned}
  P(A > B \cap B > C) &= P(A = 4) \cdot P(C = 2) \\
  &= P(A > B) \cdot P(B > C)
  \end{aligned}
  $$

  Intuitivamente: como o resultado de $B$ não importa para $A > B$ nem para $B > C$, os eventos são independentes.

* Não:

  $$
  \begin{aligned}
    P(B > C \cap C > D) 
    &= P(C = 2 \cap [C = 6 \cup (C = 2 \cap D = 1)]) \\
    &= P((C = 2 \cap C = 6) \cup (C = 2 \cap D = 1)) \\
    &= P(C = 2 \cap D = 1) \\
    &= 2/3 \cdot 1/2 \\
    &= 1/3
  \end{aligned}
  $$

  mas
  
  $$
  P(B > C) \cdot P(C > D) = 4/9
  $$


### 33. Amigos de Alice e Bob {- #amigos}

::: {.rmdbox latex=1}

* Alice, Bob, e mais $100$ pessoas vivem em uma cidade.

* $C$ é o conjunto das outras $100$ pessoas.

* $A \subseteq C$ é o conjunto de amigos de Alice.

* $B \subseteq C$ é o conjunto de amigos de Bob.

* Para cada pessoa em $C$, a probabilidade de Alice ser amiga da pessoa é $1/2$.

* Idem para Bob.

* As amizades são independentes.

:::

::: {.rmdbox latex=1}

a. Seja $D \subseteq C$. Achar $P(A = D)$.

:::

* Para cada $x \in C$:

  1. $x \in A \land x \in D$, com probabilidade $\frac12 \cdot \frac{|D|}{|C|}$, 
  
     ou (exclusivo)
     
  2. $x \not\in A \land x \not\in D$, com probabilidade $\frac12 \cdot \left(1 - \frac{|D|}{|C|}\right)$
  
* Somando as probabilidades dos casos, para cada $x \in C$, a probabilidade de $x$ estar em $A$ e em $D$, ou de $x$ não estar nem em $A$, nem em $D$, é

  $$
  \frac12 \cdot \frac{|D|}{|C|} + \frac12 \cdot \left(1 - \frac{|D|}{|C|}\right) = \frac12
  $$

* A probabilidade de $A = D$ é a probabilidade de, para todo $x \in C$, acontecer de $x \in A \land x \in D$ ou $x \in A \land x \in D$. Como os eventos são independentes, temos

  $$
  P(A = D) = \frac1{2^{|C|}}
  $$

* Vamos simular a situação. Estamos supondo que os elementos de $D$ são escolhidos segundo uma amostragem simples uniforme, sem reposição. [Veja explicações mais detalhadas abaixo](#amostra).

    ```{r attr.source='id="simular-amigos"'}
    simular <- function(p, n) {
      
      A <- (1:n)[runif(n) <= p]
      D <- (1:n)[runif(n) <= 1/2]
      
      if (length(A) != length(D)) {
        FALSE
      } else {
        all(A == D)
      }
      
    }
    ```

* Para as probabilidades não ficarem tão pequenas, vamos usar um universo $C$ com $10$ elementos apenas:

    ```{r cache=TRUE, collapse=FALSE, attr.source='id="simular-com-p"'}
    p <- 1/2
    n <- 10
    nsims <- 1e6
    
    resultado <- mean(
      1:nsims %>% 
        map_lgl(~simular(p, n))
    )
    
    cat('P(A = D) simulado = ', resultado)
    cat('P(A = D) teórico  = ', 1/2^n)
    ```


* E se $p \neq 1/2$?

* [Como explicado abaixo](#amostra), a probabilidade de um $x \in C$ pertencer a $D$ (supondo que $D$ é obtido por amostragem simples) é $1/2$.

* Então, Para cada $x \in C$:

  1. $x \in A \land x \in D$, com probabilidade $p/2$, 
  
     ou (exclusivo)
     
  2. $x \not\in A \land x \not\in D$, com probabilidade $(1 - p)/2$
  
* Somando as probabilidades dos casos, para cada $x \in C$, a probabilidade de $x$ estar em $A$ e em $D$, ou de $x$ não estar nem em $A$, nem em $D$, é, de novo, $1/2$.

* Logo, $P(A = D) = 1/ 2^{|C|}$ novamente.

* Ou seja, [o resultado vale para qualquer valor de $p$.]{.hl} Altere o valor de $p$ no [código da simulação acima](#simular-com-p) para verificar.

::: {.rmdbox latex=1}

b. Achar $P(A \subseteq B)$

:::

* Para cada $x \in A$, a probabilidade de $x \in B$ é $1/2$.

* Logo, $P(A \subseteq B) = 1 / 2^{|A|}$.

::: {.rmdbox latex=1}

c. Achar $P(A \cup B) = C$.

:::

* Para cada $x \in C$,

  $$
  \begin{aligned}
  P(x \in A \cup B) 
  &= P(x \in A) + P(x \in B) - P(x \in A \cap B) \\
  &= 1/2 + 1/2 - 1/4 \\
  &= 3/4
  \end{aligned}
  $$

* Logo, $P(A \cup B = C) = (3/4)^{|C|}$


### Extra: amostragem simples {- #amostra}

* [Na resolução acima](#amigos), usamos a igualdade

  $$
  P(x \in D) = \frac{|D|}{n}
  $$

  para representar a probabilidade de que um elemento qualquer de um universo com $n$ elementos pertença a um [conjunto $D$ fixo]{.hl}.
  
* Mas e quando não sabemos qual é o conjunto $D$, nem qual o seu tamanho?

* Neste caso, precisamos supor algo sobre a distribuição dos subconjuntos.

* O mais comum é supor que [cada um dos $2^n$ subconjuntos tem a mesma probabilidade de ser o resultado da amostragem.]{.hl}

* Com esta suposição, como calculamos $P(x \in D)$?

* Vamos condicionar ao tamanho do conjunto $D$ e marginalizar:

  $$
  P(x \in D) = \sum_{k = 0}^{n} P(x \in D \mid |D| = k) \cdot P(|D| = k)
  $$
  
* Calculando a primeira probabilidade dentro do somatório:

  $$
  \begin{aligned}
  P(x \in D \mid |D| = k) 
  &= \frac{\binom{n - 1}{k - 1}}{\binom n k} \\
  &= \frac k n
  \end{aligned}
  $$

  Na primeira linha, o numerador é a quantidade de subconjuntos de $k$ elementos que contêm $x$ (basta escolher os outros $k - 1$ elementos dentre os $n - 1$ outros elementos do universo); o denominador é o total de subconjuntos de $k$ elementos. 
  
  Perceba que [aqui usamos a suposição de que todos os subconjuntos têm a mesma probabilidade de ser amostrados]{.hl}.
  
  Perceba também que, como antes, esta probabilidade é igual a $\frac{|D|}{n}$.
  
* Calculando a segunda probabilidade dentro do somatório:

  $$
  \begin{aligned}
  P(|D| = k)
  &= \frac{\binom{n}{k}}{2^n}
  \end{aligned}
  $$

  De novo, usando a suposição de que todos os subconjuntos são equiprováveis, temos a quantidade de subconjuntos de $k$ elementos sobre o total de subconjuntos.
  
* Juntando tudo:

  $$
  \begin{aligned}
  P(x \in D) 
  &= \sum_{k = 0}^{n} P(x \in D \mid |D| = k) \cdot P(|D| = k) \\
  &= \sum_{k = 0}^{n} \frac 1 {2^n} \binom n k \frac k n \\
  &= \frac 1 {2^n} \sum_{k = 0}^{n} \binom{n - 1}{k - 1} \\
  &= \frac 1 {2^n} \cdot 2^{n - 1} \\
  &= \frac 1 2
  \end{aligned}
  $$

  ::: {.rmdimportant latex=1}
  
  Fazer amostragem uniforme significa que todos os subconjuntos são equiprováveis, [ou, equivalentemente, que cada elemento do universo tem $50\%$ de chance de estar na amostra.]{.hl}
  
  :::

* Por isso, [na simulação](#simular-amigos), geramos o conjunto $D$ com o comando 

  `D <- (1:n)[runif(n) <= 1/2]`


### 35. Xadrez {-}

::: {.rmdbox latex=1}

* Você vai jogar $2$ partidas de xadrez contra um adversário desconhecido. 

* O nível do seu adversário pode ser novato, intermediário, ou avançado, com probabilidades iguais.

* As probabilidades de você vencer uma partida são, dependendo do nível do adversário, respectivamente, $90\%$, $50\%$, e $30\%$.

a. Qual a probabilidade de você vencer a primeira partida?

b. Parabéns, você venceu a primeira partida. Dada esta informação, qual a probabilidade de que você também vença a segunda partida (suponha que, [dado o nível do seu adversário]{.hl}, os resultados das partidas são independentes)?

c. Explique a diferença entre 

   1. supor que os resultados das partidas são independentes, e
   
   2. supor que os resultados das partidas são independentes, dado o nível do seu adversário.
   
   Qual destas suposições parece mais razoável? Por quê?

:::

* Antes de mais nada, vamos definir os eventos:

  $$
  \begin{aligned}
    N &= \text{o adversário é novato} \\
    I &= \text{o adversário é intermediário} \\
    A &= \text{o adversário é avançado} \\
    V_1 &= \text{você vence a primeira partida} \\
    V_2 &= \text{você vence a segunda partida} 
  \end{aligned}
  $$

* O enunciado dá as probabilidades

  $$
  \begin{aligned}
  P(N) &= 1/3 \\
  P(I) &= 1/3 \\
  P(A) &= 1/3 \\
  P(V_1 \mid N) &= 9/10 \\
  P(V_1 \mid I) &= 5/10 \\
  P(V_1 \mid A) &= 3/10
  \end{aligned}
  $$

* Para resolver (a), basta usar probabilidade total, condicionando sobre o nível do adversário:

  $$
  \begin{aligned}
  P(V_1) 
  &= 
  P(V_1 \mid N) \cdot P(N) \;+\;
  P(V_1 \mid I) \cdot P(I) \;+\;
  P(V_1 \mid A) \cdot P(A)
  \\
  &= 
  \frac{9}{10} \cdot \frac{1}{3} + 
  \frac{5}{10} \cdot \frac{1}{3} + 
  \frac{3}{10} \cdot \frac{1}{3} \\
  &= 
  \frac{1}{3} \cdot \left(
  \frac{9}{10} + 
  \frac{5}{10} + 
  \frac{3}{10} \right) \\
  &=
  \frac{17}{30}
  \end{aligned}
  $$

  Faz sentido. Como as probabilidades dos níveis possíveis do adversário são iguais, a probabilidade de vencer é [a média aritmética]{.hl} das probabilidades de vencer contra cada nível.
  
  Se as probabilidades dos níveis do adversário fossem diferentes, seria a [média ponderada.]{.hl}
  
* Para (b), queremos calcular $P(V_2 \mid V_1)$.

  Dizer que $V_1$ e $V_2$ são independentes [dado o nível do adversário]{.hl} é dizer
  
  $$
  P(V_2 \mid V_1, N) = P(V_1 \mid V_2, N) = P(V_2 \mid N) = P(V_1 \mid N)
  $$

  e analogamente para probabilidades condicionadas a $I$ e a $A$.
  
  Ou seja, dado um nível específico do adversário, saber que $V_1$ ocorreu não altera a probabilidade de $V_2$ ocorrer, e vice-versa.
  
  Vamos calcular $P(V_2 \mid V_1)$ usando probabilidade total, condicionando ao nível:
  
  $$
  \begin{aligned}
  \underbrace{P(V_2 \mid V_1, N) \cdot P(N \mid V_1)}_{\text{novato}}
  \;+\;
  \underbrace{P(V_2 \mid V_1, I) \cdot P(I \mid V_1)}_{\text{intermediário}}
  \;+\;
  \underbrace{P(V_2 \mid V_1, A) \cdot P(A \mid V_1)}_{\text{avançado}}
  \end{aligned}
  $$

  Para o lado esquerdo de cada produto, a independência condicional diz que
  
  \begin{alignat*}{3}
  P(V_2 \mid V_1, N) &= P(V_1 \mid N) &= 9/10 \\
  P(V_2 \mid V_1, I) &= P(V_1 \mid I) &= 5/10 \\
  P(V_2 \mid V_1, A) &= P(V_1 \mid A) &= 3/10
  \end{alignat*}

  Para o lado direito de cada produto, usamos Bayes. Todas as probabilidades envolvidas já foram calculadas.
  
  Novato:
  
  $$
  \begin{aligned}
  P(N \mid V_1) 
  &=
  \frac{P(V_1 \mid N) \cdot P(N)}{P(V_1)}
  \\
  &= \frac{9/10 \cdot 1/3}{17/30}
  \\
  &= \frac{9}{17}
  \end{aligned}
  $$

  Intermediário:
  
  $$
  \begin{aligned}
  P(I \mid V_1) 
  &=
  \frac{P(V_1 \mid I) \cdot P(I)}{P(V_1)}
  \\
  &= \frac{5/10 \cdot 1/3}{17/30}
  \\
  &= \frac{5}{17}
  \end{aligned}
  $$
  
  Avançado:
  
  $$
  \begin{aligned}
  P(A \mid V_1) 
  &=
  \frac{P(V_1 \mid A) \cdot P(A)}{P(V_1)}
  \\
  &= \frac{3/10 \cdot 1/3}{17/30}
  \\
  &= \frac{3}{17}
  \end{aligned}
  $$

  A resposta final é
  
  $$
  \frac{9}{10} \cdot \frac{9}{17} \;+\;
  \frac{5}{10} \cdot \frac{5}{17} \;+\;
  \frac{3}{10} \cdot \frac{3}{17}
  \;=\;
  \frac{`r (81 + 25 + 9)/5`}{`r 170/5`}
  $$

* Para responder (c):

  Dizer que $V_1$ e $V_2$ são [incondicionalmente]{.hl} independentes seria dizer que 
  
  $$
  P(V_2 \mid V_1) = P(V_2)
  $$

  Ainda mais, como o nível do adversário não muda de uma partida para outra, teríamos também 
  
  $$
  P(V_2) = P(V_1)
  $$

  Ou seja, cada partida seria uma prova de Bernoulli com a mesma probabilidade de sucesso.
  
  Considerando independência [condicional]{.hl}, saber que vencemos a primeira partida nos dá informação sobre o nível do adversário, e esta informação é considerada para calcular a probabilidade de vencer a segunda partida.
  
  De fato, usando independência condicional, temos
  
  $$
  P(V_1) \approx `r 17/30`
  $$
  
  e
  
  $$
  P(V_2 \mid V_1) \approx `r 23/34`
  $$
  

### 36. [Paradoxo de Berkson](https://en.wikipedia.org/wiki/Berkson%27s_paradox) {-}

::: {.rmdbox latex=1}

[Se]{.hl} 

* $A$ e $B$ são independentes, e

* $P(A \cap B) > 0$, e

* $P(A \cup B) < 1$, e

* $C = A \cup B$

[então]{.hl} $A$ e $B$ são condicionalmente dependentes, dado $C$, com

$$
P(A \mid B, C) < P(A \mid C)
$$

Exemplo: universidade admite candidatos que são bons jogadores de basquete ($A$) ou que são bons em Matemática ($B$) --- [supondo que estes são eventos independentes, o que é meio duvidoso]{.hl}.^[Na verdade, em vez de independência entre $A$ e $B$, basta que um evento atrapalhe o outro para termos o paradoxo. Em termos da [nossa medida de independência $I$](#i), basta $0< I < 1$.] 

:::

* De $P(A \cap B) > 0$, temos que $P(A) > 0$ e que $P(B) > 0$.

* Daí, $P(C) = P(A \cup B) > 0$.

* Para o lado esquerdo:

  $$
  \begin{aligned}
  P(A \mid B, C) 
  &= \frac{P(A \cap B \cap C)}{P(B \cap C)} \\
  &= \frac{P(A \cap B)}{P(B)} \\
  &= \frac{P(A)P(B)}{P(B)} \\
  &= P(A)
  \end{aligned}
  $$

* Para o lado direito:

  $$
  \begin{aligned}
  P(A \mid C) 
  &= \frac{P(A \cap C)}{P(C)} \\
  &= \frac{P(A)}{P(C)} \\
  &= \frac{P(A)}{P(A \cup B)} 
  \end{aligned}
  $$

* Como o denominador $P(A \cup B) < 1$, esta probabilidade é maior do que $P(A)$.


### 37. Doenças e sintoma {-}

::: {.rmdbox latex=1}

* Quem tem a doença $D_1$ ou a doença $D_2$ (ou ambas) tem o sintoma estranho $W$.

* $D_1$ e $D_2$ são independentes, com $P(D_j) = p_j$, e com $q_j = 1 - p_j$. 

* $0 < p_j < 1$.

* Uma pessoa sadia tem o sintoma $W$ com probabilidade $w_0$.

:::

::: {.rmdbox latex=1}

a. Achar $P(W)$.

:::

* Por probabilidade total:

  $$
  \begin{aligned}
  P(W) 
  &= P(W \mid D_1 \cup D_2) \cdot P(D_1 \cup D_2) +
  P(W \mid \neg(D_1 \cup D_2)) \cdot P(\neg(D_1 \cup D_2)) \\
  &= 1 \cdot (p_1 + p_2 - p_1p_2) + w_0 \cdot (1 - p_1 - p_2 + p_1p_2) \\
  &= p_1 + p_2 - p_1p_2 + w_0q_1q_2 
  \end{aligned}
  $$

::: {.rmdbox latex=1}

b. Achar $P(D_1 \mid W)$, $P(D_2 \mid W)$, e $P(D_1 \cap D_2 \mid W)$.

:::

* Por Bayes:

  $$
  \begin{aligned}
    P(D_1 \mid W) 
    &= \frac{P(W \mid D_1) \cdot P(D_1)}{P(W)} \\
    &= \frac{1 \cdot p_1}{p_1 + p_2 - p_1p_2 + w_0q_1q_2} \\
    &= \frac{p_1}{p_1 + p_2 - p_1p_2 + w_0q_1q_2}
  \end{aligned}
  $$

* Analogamente,

  $$
  P(D_2 \mid W) = \frac{p_2}{p_1 + p_2 - p_1p_2 + w_0q_1q_2}
  $$

* Finalmente,

  $$
  \begin{aligned}
  P(D_1 \cap D_2 \mid W)
  &= \frac{P(D_1 \cap D_2 \cap W)}{P(W)} \\
  &= \frac{P(D_1 \cap D_2)}{P(W)} & (\text{pois }D_1 \cap D_2 \subseteq W)\\
  &= \frac{p_1p_2}{p_1 + p_2 - p_1p_2 + w_0q_1q_2}
  \end{aligned}
  $$

::: {.rmdbox latex=1}

c. $D_1$ e $D_2$ são condicionalmente independentes, dado $W$?

:::

* Basta verificar se

  $$
  P(D_1 \mid W) \cdot P(D_2 \mid W) = P(D_1 \cap D_2 \mid W)
  $$

* Isto equivale a

  $$
  \frac{p_1p_2}{(p_1 + p_2 - p_1p_2 + w_0q_1q_2)^2} = 
  \frac{p_1p_2}{p_1 + p_2 - p_1p_2 + w_0q_1q_2}
  $$

* [Esta igualdade só é verdade se]{.hl}

  1. $w_0 = 0$ e $P(D_1 \cup D_2) = 0$ (o que é proibido pelo enunciado). Ninguém estaria doente, e ninguém teria o sintoma.
  
     ou
  
  2. $P(D_1 \cup D_2) = 1$. Aqui, todos estariam doentes, e todos teriam o sintoma. 
  
* Lembramos que 

  $$
  w_0 = P(W \mid \neg(D_1 \cup D_2)) = 
  \frac{P(W \cap \neg(D_1 \cup D_2))}{P(\neg(D_1 \cup D_2))}
  $$

* No caso (1) acima, $w_0 = P(W) = 0$, e não faz mais sentido condicionar sobre $W$.

* No caso (2) acima, $P(\neg(D_1 \cup D_2)) = 0$, e a própría definição de $w_0$ deixa de fazer sentido.

  
::: {.rmdbox latex=1}

d. Suponha que $w_0 = 0$. Neste caso, $D_1$ e $D_2$ são condicionalmente independentes, dado $W$?

:::

* Por causa do discutido no item (c), vamos supor que $$0 < P(D_1 \cup D_2) < 1$$ para que possamos condicionar sobre $W$ e para que a definição de $w_0$ faça sentido.

* Como no item (c), verificamos se

  $$
  P(D_1 \mid W) \cdot P(D_2 \mid W) = P(D_1 \cap D_2 \mid W)
  $$

* Que equivale a 

  $$
  \frac{p_1p_2}{(p_1 + p_2 - p_1p_2)^2} = 
  \frac{p_1p_2}{p_1 + p_2 - p_1p_2}
  $$

* Isto é impossível, se $0 < P(D_1 \cup D_2) < 1$.

* Intuitivamente, $D_1$ e $D_2$ seriam [condicionalmente dependentes]{.hl}, dado $W$, pois, dado que uma pessoa tem o sintoma $W$, saber que ela não tem a doença $D_1$ imediatamente nos diz que ela tem a doença $D_2$.

::: {.rmdnote latex=1}

Quando $w_0 = 0$ e $0 < P(D_1 \cup D_2) < 1$, a consequência é que 

$$
D_1 \cup D_2 = W
$$
como eventos.

Daí, com as outras condições do enunciado, temos uma instância do [paradoxo de Berkson](#paradoxo-de-berkson), com $A = D_1$ e $B = D_2$.

:::


### 38. Naïve Bayes {-}

::: {.rmdbox latex=1}

* Temos uma lista de $100$ palavras.

* Evento $W_j = {}$ a palavra $j$ aparece no *email*.

* Evento $\text{spam} = {}$ o *email* é *spam*.

* $p = P(\text{spam})$.

* $p_j = P(W_j \mid \text{spam})$.

* $r_j = P(W_j \mid \neg \text{spam})$.

* A *naïveté* do algoritmo consiste nas seguintes suposições --- não-realistas, mas úteis:

  * Os $W_j$ são condicionalmente independentes, dado $\text{spam}$.
  
  * Os $W_j$ são condicionalmente independentes, dado $\neg \text{spam}$.

* Um novo *email* é recebido. Contém as palavras $23$, $64$, e $65$, e nenhuma das outras.

* Vamos chamar de $\vec W$ a lista de eventos

  $$
  \begin{array}{l}
  \neg W_1, \ldots, \neg W_{22}, \\
  W_{23},\\ 
  \neg W_{24}, \ldots, \neg W_{63},\\    
  W_{64}, W_{65},\\ 
  \neg W_{66}, \ldots, \neg W_{100}
  \end{array}
  $$
  
* Calcular $P\left( \text{spam} \;\middle|\; \vec W \right)$.

:::

* Por Bayes:

  $$
  P\left( \text{spam} \;\middle|\; \vec W \right) = 
  \frac{P\left(\vec W \;\middle|\; \text{spam} \right)}
  {P\left( \vec W  \right)}
  $$

* Com a suposição *naïve*, podemos multiplicar as probabilidades condicionais:

  $$
  P\left(\vec W\;\middle|\; \text{spam} \right) =
  p_{23} \cdot p_{64} \cdot p_{65} \cdot
  \!\!\!\!\!\!\!\!\!
  \prod_{
    \begin{array}{c}
    1 \leq j \leq 100, \\
    j \not\in \{ 23, 64, 65 \}
    \end{array}
  }
  \!\!\!\!\!\!\!
  (1 - p_j)
  $$
  
  [Vamos chamar este valor de $m$.]{.hl}
  
* Para calcular $P\left( \vec W \right)$, usamos probabilidade total:

  $$
  \begin{aligned}
  P\left( \vec W \right) 
  &= 
    P\left(\vec W \;\middle|\; \text{spam} \right) 
    \cdot P(\text{spam})
    \;+\;
    P\left(\vec W \;\middle|\; \neg\text{spam} \right) 
    \cdot P(\neg\text{spam})
  \\
  &=
    m \cdot p \;+\; m' \cdot (1 - p)
  \end{aligned}
  $$
  
  onde, de maneira análoga ao cálculo de $m$,
  
  $$
  m' = 
    r_{23} \cdot r_{64} \cdot r_{65} \cdot
  \!\!\!\!\!\!\!\!\!
  \prod_{
    \begin{array}{c}
    1 \leq j \leq 100. \\
    j \not\in \{ 23, 64, 65 \}
    \end{array}
  }
  \!\!\!\!\!\!\!
  (1 - r_j)
  $$
  
* O valor procurado é, então,

  $$
  P\left( \text{spam} \;\middle|\; \vec W \right) 
  = 
  \frac{P\left(\vec W \;\middle|\; \text{spam} \right)}
  {P\left( \vec W  \right)}
  = \frac{mp}{mp + m'(1 - p)}
  $$
  
  
